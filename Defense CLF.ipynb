{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset and Coaching Data\n",
    "data = pd.read_csv('Data/Final Dataset.csv')\n",
    "\n",
    "coaching_data = pd.read_csv('Data/Coaching Data.csv', skiprows = [0,1], skipfooter = 202)\n",
    "coaching_data = coaching_data.rename(columns = {'FBS Team': 'Team'})\n",
    "coaching_data = coaching_data[['Team','2019','2020','2021','2022','2023','2024']]\n",
    "coaching_data = coaching_data.melt(id_vars='Team', var_name = 'Season', value_name = 'Coach')\n",
    "coaching_data['Season'] = coaching_data['Season'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Coaching Change Function to Add Coaching Change Column to Data\n",
    "def coach_change(row, data):\n",
    "\n",
    "    team = str(row.Team)\n",
    "    season = int(row.Season)\n",
    "\n",
    "    try:\n",
    "    \n",
    "        curr_coach = data[(data['Team'] == team) & (data['Season'] == season)]['Coach']\n",
    "        curr_coach = curr_coach[curr_coach.index[0]]\n",
    "        next_coach = data[(data['Team'] == team) & (data['Season'] == season + 1)]['Coach']\n",
    "        next_coach = next_coach[next_coach.index[0]]\n",
    "\n",
    "        if curr_coach != next_coach:\n",
    "            return 'Yes'\n",
    "        return 'No'\n",
    "        \n",
    "    \n",
    "    except:\n",
    "        return 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add coaching change info to data\n",
    "data['Coach Change'] = data.apply(lambda x: coach_change(x, coaching_data), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RB', 'WR', 'QB', 'TE', 'LB', 'DB', 'OL', 'DL', 'CB', 'S', 'PK',\n",
       "       'LS', 'P', 'DT', 'DE', 'FB', 'C', 'OT', 'G', 'NT', 'ATH', 'OLB',\n",
       "       '?'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions = data.Position.unique()\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add recruiting info\n",
    "for i,year in enumerate(['2015','2016','2017','2018', '2019','2020', '2021', '2022', '2023']):\n",
    "    file = pd.read_csv('Data/Player Recruit Ranking/' + year + '.csv')\n",
    "    file.rename(columns = {'AthleteId': 'PlayerId', 'Year': 'Class of'}, inplace = True)\n",
    "    data = data.merge(file, on = 'PlayerId', how = 'left', suffixes = [None, '_' + str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Columns\n",
    "for i in range(0,9):\n",
    "    data['Stars'] = data['Stars'].combine_first(data['Stars_' + str(i)])\n",
    "    data.drop(columns = ['Stars_' + str(i)], inplace = True)\n",
    "for i in range(1,9):\n",
    "    data['Rating'] = data['Rating'].combine_first(data['Rating_' + str(i)])\n",
    "    data['Ranking'] = data['Ranking'].combine_first(data['Ranking_' + str(i)])\n",
    "    data['Class of'] = data['Class of'].combine_first(data['Class of_' + str(i)])\n",
    "    data.drop(columns = ['Rating_' + str(i)], inplace = True)\n",
    "    data.drop(columns = ['Ranking_' + str(i)], inplace = True)\n",
    "    data.drop(columns = ['Class of_' + str(i)], inplace = True)\n",
    "\n",
    "data['Yr'] = data['Season'] - data['Class of'] + 1\n",
    "data.drop(columns = ['Class of'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_groups = {'OL':['OL', 'NT', 'OT', 'G', 'C','FB'],\n",
    "                   'TE':['TE'],\n",
    "                   'QB':['QB'],\n",
    "                   'RB':['RB'],\n",
    "                   'WR':['WR'],\n",
    "                   'DL':['DT', 'DE', 'DL'],\n",
    "                   'DB':['DB', 'CB', 'S'],\n",
    "                   'LB':['LB'],\n",
    "                   'ST':['LS', 'P', 'PK']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate Data into Position Groups\n",
    "data_sets = {}\n",
    "for key in position_groups.keys():\n",
    "    data_sets[key] = data.copy()[data.copy()['Position'].isin(position_groups[key])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Defensive data as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL = data_sets['DL'].copy().drop(columns = ['PositionId', 'ConferenceId', 'TeamId'])\n",
    "DB = data_sets['DB'].copy().drop(columns = ['PositionId', 'ConferenceId', 'TeamId'])\n",
    "LB = data_sets['LB'].copy().drop(columns = ['PositionId', 'ConferenceId', 'TeamId'])\n",
    "\n",
    "defense = pd.concat([DL,DB,LB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "defense['Stars'] = defense['Stars'].fillna(0)\n",
    "defense['Yr'] = defense['Yr'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Engineer PCT Features\n",
    "defense['Pct_Team_INT'] = defense['INT']/defense['Team interceptions']\n",
    "defense['Pct_Team_SACKS'] = defense['SACKS']/defense['Team sacks']\n",
    "defense['Pct_Team_TFL'] = defense['TFL']/defense['Team tacklesForLoss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Improve Function\n",
    "def improve(row, column, data):\n",
    "    try: \n",
    "        id = int(row['PlayerId'])\n",
    "        season = int(row['Season'])\n",
    "        column = column\n",
    "        imp = float(data[(data['PlayerId'] == id) & (data['Season'] == season)][column]) > float(data[(data['PlayerId'] == id) & (data['Season'] == season-1)][column])\n",
    "\n",
    "        if imp:\n",
    "            return 'Yes' \n",
    "        return 'No'\n",
    "    except:\n",
    "        return 'Yes'\n",
    " \n",
    "#Function to compare player stats with players of same year and star ranking\n",
    "def compare(row, column, data):\n",
    "    \n",
    "    star = int(row['Stars'])\n",
    "    year = int(row['Yr'])\n",
    "    stat = column\n",
    "    pos = str(row['Position'])\n",
    "    id = int(row['PlayerId'])\n",
    "    season = int(row['Season'])\n",
    "\n",
    "    mean = data[(data['Stars'] == star) & (data['Yr'] == year) & (data['Position'] == pos)][stat].mean()\n",
    "    p_stat = data[(data['PlayerId'] == id)&(data['Season'] == season)][stat]\n",
    "    p_stat = p_stat[p_stat.index[0]]\n",
    "\n",
    "    if int(p_stat>mean):\n",
    "        return 1\n",
    "    elif int(p_stat<mean):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Engineer Improve Features\n",
    "feats = ['FUM', 'INT', 'PD', 'QB HUR', 'SACKS', 'SOLO', 'Pct_Team_INT', 'Pct_Team_SACKS', 'Pct_Team_TFL']\n",
    "\n",
    "for feat in feats:\n",
    "    defense[feat+'_improve'] = defense.apply(lambda x: improve(x, feat, defense), axis = 1)\n",
    "\n",
    "# for feat in feats:\n",
    "#     defense[feat + '_compare'] = defense.apply(lambda x: compare(x, feat, defense), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify Relevant Columns\n",
    "#No team stats added as of right now\n",
    "rel_feats = ['FUM', 'INT', 'PD', 'QB HUR', 'SACKS', 'SOLO', 'Pct_Team_INT', 'Pct_Team_SACKS', 'Pct_Team_TFL',\n",
    "             'FUM_improve', 'INT_improve', 'PD_improve', 'QB HUR_improve', 'SACKS_improve', 'SOLO_improve', \n",
    "             'Pct_Team_INT_improve', 'Pct_Team_SACKS_improve', 'Pct_Team_TFL_improve', \n",
    "             'Team','Conference','Position','Yr','Stars', 'Coach Change', 'Ranking', 'Rating', 'Transfer_Portal']\n",
    "\n",
    "# 'FUM_compare', 'INT_compare', 'PD_compare', 'QB HUR_compare', 'SACKS_compare', 'SOLO_compare', \n",
    "#              'Pct_Team_INT_compare', 'Pct_Team_SACKS_compare', 'Pct_Team_TFL_compare', \n",
    "\n",
    "\n",
    "#Get rid of 2019, narrow down to relevant features\n",
    "defense = defense[defense['Season'] != 2019]\n",
    "defense = defense[rel_feats]\n",
    "#Encode Label Columns\n",
    "defense['Transfer_Portal'] = np.where(defense['Transfer_Portal'].values == 'Yes', 1, 0)\n",
    "\n",
    "#Convert Yr and Stars to Categorical Variables\n",
    "defense['Yr'] = defense['Yr'].astype('str')\n",
    "defense['Stars'] = defense['Stars'].astype('str')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into Training and Testing Data\n",
    "X = defense.copy().drop(columns=['Transfer_Portal'])\n",
    "y = defense['Transfer_Portal']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n",
    "\n",
    "#Specify the Numerical Features and Categorical Features\n",
    "categorical = ['FUM_improve', 'INT_improve', 'PD_improve', 'QB HUR_improve', 'SACKS_improve', 'SOLO_improve', \n",
    "             'Pct_Team_INT_improve', 'Pct_Team_SACKS_improve', 'Pct_Team_TFL_improve', \n",
    "             'Team','Conference','Position','Yr','Stars', 'Coach Change']\n",
    "\n",
    "numerical = [feat for feat in X.columns if feat not in categorical]\n",
    "\n",
    "#Pipeline to SimpleImpute and OneHot Encode Categorical Features (Training data only)\n",
    "impute_encode = Pipeline([('impute',SimpleImputer(strategy='constant',fill_value='N/A')), ('encode',OneHotEncoder(handle_unknown='ignore'))])\n",
    "column_transform = ColumnTransformer([('cat_encode', impute_encode, categorical), ('numerical_pass', SimpleImputer(strategy='constant',fill_value=0),numerical)])\n",
    "\n",
    "X_train = column_transform.fit_transform(X_train)\n",
    "\n",
    "#Fit Pipeline ColumnTransformer to testing features\n",
    "X_test = X_test.fillna(0)\n",
    "X_test = column_transform.transform(X_test)\n",
    "\n",
    "#SMOTE Balancing of Training Data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train,y_train)\n",
    "X_train = X_train.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Scores:\n",
      "0.7241111217815432\n",
      "0.8124853290424575\n",
      "0.7129281568797088\n",
      "0.6591303979261813\n",
      " \n",
      "Precision Scores:\n",
      "0.9027158376214981\n",
      "0.7621081582987995\n",
      "0.933018372703412\n",
      "0.6174868439166076\n",
      " \n",
      "Recall Scores:\n",
      "0.6686196247141174\n",
      "0.9204771535425715\n",
      "0.6587435240496464\n",
      "0.8825462728803517\n"
     ]
    }
   ],
   "source": [
    "#CV Models w/ Training Data\n",
    "cv = KFold(n_splits = 5)\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def classifiers():\n",
    "    gb_clf = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=42))\n",
    "    SVM_clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "    forest_clf = RandomForestClassifier(random_state=42)\n",
    "    log_clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=10000, class_weight='balanced'))\n",
    "\n",
    "    gb_f1 = cross_val_score(gb_clf, X_train, y_train, cv=cv, scoring='f1')\n",
    "    SVM_f1 = cross_val_score(SVM_clf, X_train, y_train, cv=cv, scoring='f1')\n",
    "    forest_f1 = cross_val_score(forest_clf, X_train, y_train, cv=cv, scoring='f1')\n",
    "    log_f1 = cross_val_score(log_clf, X_train, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "    gb_pre = cross_val_score(gb_clf, X_train, y_train, cv=cv, scoring='precision')\n",
    "    SVM_pre = cross_val_score(SVM_clf, X_train, y_train, cv=cv, scoring='precision')\n",
    "    forest_pre = cross_val_score(forest_clf, X_train, y_train, cv=cv, scoring='precision')\n",
    "    log_pre = cross_val_score(log_clf, X_train, y_train, cv=cv, scoring='precision')\n",
    "\n",
    "    gb_re = cross_val_score(gb_clf, X_train, y_train, cv=cv, scoring='recall')\n",
    "    SVM_re = cross_val_score(SVM_clf, X_train, y_train, cv=cv, scoring='recall')\n",
    "    forest_re = cross_val_score(forest_clf, X_train, y_train, cv=cv, scoring='recall')\n",
    "    log_re = cross_val_score(log_clf, X_train, y_train, cv=cv, scoring='recall')\n",
    "\n",
    "    print('F1 Scores:')\n",
    "    print(gb_f1.mean())\n",
    "    print(SVM_f1.mean())\n",
    "    print(forest_f1.mean())\n",
    "    print(log_f1.mean())\n",
    "    print(' ')\n",
    "    print('Precision Scores:')\n",
    "    print(gb_pre.mean())\n",
    "    print(SVM_pre.mean())\n",
    "    print(forest_pre.mean())\n",
    "    print(log_pre.mean())\n",
    "    print(' ')\n",
    "    print('Recall Scores:')\n",
    "    print(gb_re.mean())\n",
    "    print(SVM_re.mean())\n",
    "    print(forest_re.mean())\n",
    "    print(log_re.mean())\n",
    "\n",
    "classifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base RandomForestClassifier Metrics\n",
      "-----------------------------------\n",
      "F1 Score:  0.18666666666666668\n",
      "Precision Score:  0.8235294117647058\n",
      "Recall Score:  0.10526315789473684\n",
      "Accuracy:  0.9465381244522348\n"
     ]
    }
   ],
   "source": [
    "#Evaluate trained model on test data\n",
    "#Use 'weighted' f1 score, recall, precision since data is imbalanced\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42, class_weight = 'balanced')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print('Base RandomForestClassifier Metrics')\n",
    "print('-----------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base GradientBoostingClassifer Metrics\n",
      "---------------------------------------\n",
      "F1 Score:  0.31446540880503143\n",
      "Precision Score:  0.9615384615384616\n",
      "Recall Score:  0.18796992481203006\n",
      "Accuracy:  0.9522348816827344\n"
     ]
    }
   ],
   "source": [
    "#Evaluate trained model on test data\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print('Base GradientBoostingClassifer Metrics')\n",
    "print('---------------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base LogisticRegressionClassifer Metrics\n",
      "---------------------------------------\n",
      "F1 Score:  0.29985855728429983\n",
      "Precision Score:  0.18466898954703834\n",
      "Recall Score:  0.7969924812030075\n",
      "Accuracy:  0.7830850131463628\n"
     ]
    }
   ],
   "source": [
    "#Evaluate trained model on test data\n",
    "#Use 'weighted' f1 score, recall, precision since data is imbalanced\n",
    "\n",
    "clf = LogisticRegression(max_iter=10000, class_weight='balanced')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print('Base LogisticRegressionClassifer Metrics')\n",
    "print('---------------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummmyClassifier Metrics\n",
      "-----------------------------------\n",
      "F1 Score:  0.10711430855315747\n",
      "Precision Score:  0.05992844364937388\n",
      "Recall Score:  0.5037593984962406\n",
      "Accuracy:  0.5105170902716915\n"
     ]
    }
   ],
   "source": [
    "#Dummy Classifier:\n",
    "\n",
    "dummy = make_pipeline(StandardScaler(), DummyClassifier(random_state=42, strategy='stratified'))\n",
    "\n",
    "dummy.fit(X_train, y_train)\n",
    "preds = dummy.predict(X_test.toarray())\n",
    "\n",
    "print('DummmyClassifier Metrics')\n",
    "print('-----------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue to test and tune the logistic regression and gradient boost models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "parameters = {\"random_state\": [42],\n",
    "              \"penalty\": ['l2'],\n",
    "              \"solver\": ['lbfgs', 'liblinear'], \n",
    "              \"C\": [1, 5, 10, 100, 1000], \n",
    "              \"max_iter\": [100, 500, 10000, 15000],\n",
    "              \"class_weight\": ['balanced']}\n",
    "\n",
    "clf_log = GridSearchCV(LogisticRegression(), parameters, scoring='recall').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LogisticRegression Metrics\n",
      "---------------------------------------\n",
      "F1 Score:  0.30346820809248554\n",
      "Precision Score:  0.18783542039355994\n",
      "Recall Score:  0.7894736842105263\n",
      "Accuracy:  0.7887817703768624\n"
     ]
    }
   ],
   "source": [
    "preds = clf_log.predict(X_test)\n",
    "\n",
    "print('Best LogisticRegression Metrics')\n",
    "print('---------------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GradientBoosting Metrics\n",
      "---------------------------------------\n",
      "F1 Score:  0.2199546485260771\n",
      "Precision Score:  0.1295060080106809\n",
      "Recall Score:  0.7293233082706767\n",
      "Accuracy:  0.6985100788781771\n"
     ]
    }
   ],
   "source": [
    "#GradientBoosting\n",
    "parameters = {\"random_state\": [42],\n",
    "              \"learning_rate\": [0.01, 0.015, 0.02,0.05, 0.075, 0.1],\n",
    "              \"n_estimators\": np.arange(1,100,5),\n",
    "              \"max_features\":['sqrt', 'log2', None]}\n",
    "\n",
    "clf_gb = GridSearchCV(GradientBoostingClassifier(), parameters, scoring='recall').fit(X_train, y_train)\n",
    "\n",
    "preds = clf_gb.predict(X_test)\n",
    "\n",
    "print('Best GradientBoosting Metrics')\n",
    "print('---------------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RandomForest Metrics\n",
      "---------------------------------------\n",
      "F1 Score:  0.4387755102040816\n",
      "Precision Score:  0.6825396825396826\n",
      "Recall Score:  0.3233082706766917\n",
      "Accuracy:  0.9517966695880806\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "parameters = {\"random_state\": [42],\n",
    "              \"n_estimators\": np.arange(1,100,5),\n",
    "              \"max_features\":['sqrt', 'log2', None], \n",
    "              \"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "              \"class_weight\": ['balanced']}\n",
    "\n",
    "clf_rf = GridSearchCV(RandomForestClassifier(), parameters, scoring='recall').fit(X_train, y_train)\n",
    "\n",
    "preds = clf_rf.predict(X_test)\n",
    "\n",
    "print('Best RandomForest Metrics')\n",
    "print('---------------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the probabilities that are given back to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = defense[defense['Transfer_Portal'] == 1].drop(columns=['Transfer_Portal'])\n",
    "X_neg = defense[defense['Transfer_Portal'] == 0].drop(columns=['Transfer_Portal'])\n",
    "\n",
    "X_pos = column_transform.transform(X_pos).toarray()\n",
    "X_neg = column_transform.transform(X_neg).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(nest_list):\n",
    "    probs = []\n",
    "    for i in nest_list:\n",
    "        probs.append(i[1])\n",
    "\n",
    "    return sum(probs)/len(nest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression probabilities of a player entering the transfer portal\n",
      "---------------------------------------\n",
      "Average probability of players who did enter:  0.7281565720944059\n",
      "Average probability of players who did NOT enter:  0.2188115170722414\n",
      " \n",
      "GB probabilities of a player entering the transfer portal\n",
      "---------------------------------------\n",
      "Average probability of players who did enter:  0.5200014947553792\n",
      "Average probability of players who did NOT enter:  0.3992106859642242\n"
     ]
    }
   ],
   "source": [
    "log_pos_proba = average(clf_log.predict_proba(X_pos))\n",
    "log_neg_proba = average(clf_log.predict_proba(X_neg))\n",
    "\n",
    "gb_pos_proba = average(clf_gb.predict_proba(X_pos))\n",
    "gb_neg_proba = average(clf_gb.predict_proba(X_neg))\n",
    "\n",
    "print('Logistic Regression probabilities of a player entering the transfer portal')\n",
    "print('---------------------------------------')\n",
    "print('Average probability of players who did enter: ', log_pos_proba)\n",
    "print('Average probability of players who did NOT enter: ', log_neg_proba)\n",
    "print(' ')\n",
    "print('GB probabilities of a player entering the transfer portal')\n",
    "print('---------------------------------------')\n",
    "print('Average probability of players who did enter: ', gb_pos_proba)\n",
    "print('Average probability of players who did NOT enter: ', gb_neg_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DefensiveCLF.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sklearn Job-lib\n",
    "joblib.dump(clf_log, \"DefensiveCLF.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
