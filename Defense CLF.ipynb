{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Killen\\AppData\\Local\\Temp\\ipykernel_44648\\736867005.py:2: DtypeWarning: Columns (44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('Data/Final Dataset.csv')\n",
      "C:\\Users\\Killen\\AppData\\Local\\Temp\\ipykernel_44648\\736867005.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  coaching_data = pd.read_csv('Data/Coaching Data.csv', skiprows = [0,1], skipfooter = 202)\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset and Coaching Data\n",
    "data = pd.read_csv('Data/Final Dataset.csv')\n",
    "\n",
    "coaching_data = pd.read_csv('Data/Coaching Data.csv', skiprows = [0,1], skipfooter = 202)\n",
    "coaching_data = coaching_data.rename(columns = {'FBS Team': 'Team'})\n",
    "coaching_data = coaching_data[['Team','2019','2020','2021','2022','2023','2024']]\n",
    "coaching_data = coaching_data.melt(id_vars='Team', var_name = 'Season', value_name = 'Coach')\n",
    "coaching_data['Season'] = coaching_data['Season'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Coaching Change Function to Add Coaching Change Column to Data\n",
    "def coach_change(row, data):\n",
    "\n",
    "    team = str(row.Team)\n",
    "    season = int(row.Season)\n",
    "\n",
    "    try:\n",
    "    \n",
    "        curr_coach = data[(data['Team'] == team) & (data['Season'] == season)]['Coach']\n",
    "        curr_coach = curr_coach[curr_coach.index[0]]\n",
    "        next_coach = data[(data['Team'] == team) & (data['Season'] == season + 1)]['Coach']\n",
    "        next_coach = next_coach[next_coach.index[0]]\n",
    "\n",
    "        if curr_coach != next_coach:\n",
    "            return 'Yes'\n",
    "        return 'No'\n",
    "        \n",
    "    \n",
    "    except:\n",
    "        return 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add coaching change info to data\n",
    "data['Coach Change'] = data.apply(lambda x: coach_change(x, coaching_data), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RB', 'WR', 'QB', 'TE', 'LB', 'DB', 'OL', 'DL', 'CB', 'S', 'PK',\n",
       "       'LS', 'P', 'DT', 'DE', 'FB', 'C', 'OT', 'G', 'NT', 'ATH', 'OLB',\n",
       "       '?'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions = data.Position.unique()\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add recruiting info\n",
    "for i,year in enumerate(['2015','2016','2017','2018', '2019','2020', '2021', '2022', '2023']):\n",
    "    file = pd.read_csv('Data/Player Recruit Ranking/' + year + '.csv')\n",
    "    file.rename(columns = {'AthleteId': 'PlayerId', 'Year': 'Class of'}, inplace = True)\n",
    "    data = data.merge(file, on = 'PlayerId', how = 'left', suffixes = [None, '_' + str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Columns\n",
    "for i in range(0,9):\n",
    "    data['Stars'] = data['Stars'].combine_first(data['Stars_' + str(i)])\n",
    "    data.drop(columns = ['Stars_' + str(i)], inplace = True)\n",
    "for i in range(1,9):\n",
    "    data['Rating'] = data['Rating'].combine_first(data['Rating_' + str(i)])\n",
    "    data['Ranking'] = data['Ranking'].combine_first(data['Ranking_' + str(i)])\n",
    "    data['Class of'] = data['Class of'].combine_first(data['Class of_' + str(i)])\n",
    "    data.drop(columns = ['Rating_' + str(i)], inplace = True)\n",
    "    data.drop(columns = ['Ranking_' + str(i)], inplace = True)\n",
    "    data.drop(columns = ['Class of_' + str(i)], inplace = True)\n",
    "\n",
    "data['Yr'] = data['Season'] - data['Class of'] + 1\n",
    "data.drop(columns = ['Class of'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_groups = {'OL':['OL', 'NT', 'OT', 'G', 'C','FB'],\n",
    "                   'TE':['TE'],\n",
    "                   'QB':['QB'],\n",
    "                   'RB':['RB'],\n",
    "                   'WR':['WR'],\n",
    "                   'DL':['DT', 'DE', 'DL'],\n",
    "                   'DB':['DB', 'CB', 'S'],\n",
    "                   'LB':['LB'],\n",
    "                   'ST':['LS', 'P', 'PK']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate Data into Position Groups\n",
    "data_sets = {}\n",
    "for key in position_groups.keys():\n",
    "    data_sets[key] = data.copy()[data.copy()['Position'].isin(position_groups[key])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Defensive data as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL = data_sets['DL'].copy().drop(columns = ['PositionId', 'ConferenceId', 'TeamId'])\n",
    "DB = data_sets['DB'].copy().drop(columns = ['PositionId', 'ConferenceId', 'TeamId'])\n",
    "LB = data_sets['LB'].copy().drop(columns = ['PositionId', 'ConferenceId', 'TeamId'])\n",
    "\n",
    "defense = pd.concat([DL,DB,LB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Season', 'PlayerId', 'Player', 'Position', 'Team', 'Conference',\n",
       "       'Usage Overall', 'Usage Pass', 'Usage Rush', 'Usage FirstDown',\n",
       "       'Usage SecondDown', 'Usage ThirdDown', 'Usage StandardDowns',\n",
       "       'Usage PassingDowns', 'ATT', 'AVG', 'CAR', 'COMPLETIONS', 'FGA', 'FGM',\n",
       "       'FUM', 'INT', 'In 20', 'LONG', 'LOST', 'NO', 'PCT', 'PD', 'PTS',\n",
       "       'QB HUR', 'REC', 'SACKS', 'SOLO', 'TB', 'TD', 'TFL', 'TOT', 'XPA',\n",
       "       'XPM', 'YDS', 'YPA', 'YPC', 'YPP', 'YPR', 'Division', 'ExpectedWins',\n",
       "       'Total Games', 'Total Wins', 'Total Losses', 'Total Ties',\n",
       "       'ConferenceGames Games', 'ConferenceGames Wins',\n",
       "       'ConferenceGames Losses', 'ConferenceGames Ties', 'HomeGames Games',\n",
       "       'HomeGames Wins', 'HomeGames Losses', 'HomeGames Ties',\n",
       "       'AwayGames Games', 'AwayGames Wins', 'AwayGames Losses',\n",
       "       'AwayGames Ties', 'Team firstDowns', 'Team fourthDownConversions',\n",
       "       'Team fourthDowns', 'Team fumblesLost', 'Team fumblesRecovered',\n",
       "       'Team games', 'Team interceptionTDs', 'Team interceptionYards',\n",
       "       'Team interceptions', 'Team kickReturnTDs', 'Team kickReturnYards',\n",
       "       'Team kickReturns', 'Team netPassingYards', 'Team passAttempts',\n",
       "       'Team passCompletions', 'Team passesIntercepted', 'Team passingTDs',\n",
       "       'Team penalties', 'Team penaltyYards', 'Team possessionTime',\n",
       "       'Team puntReturnTDs', 'Team puntReturnYards', 'Team puntReturns',\n",
       "       'Team rushingAttempts', 'Team rushingTDs', 'Team rushingYards',\n",
       "       'Team sacks', 'Team tacklesForLoss', 'Team thirdDownConversions',\n",
       "       'Team thirdDowns', 'Team totalYards', 'Team turnovers', 'Stars',\n",
       "       'Transfer_Portal', 'Coach Change', 'Ranking', 'Rating', 'Yr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defense.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Engineer PCT Features\n",
    "defense['Pct_Team_INT'] = defense['INT']/defense['Team interceptions']\n",
    "defense['Pct_Team_SACKS'] = defense['SACKS']/defense['Team sacks']\n",
    "defense['Pct_Team_TFL'] = defense['TFL']/defense['Team tacklesForLoss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Improve Function\n",
    "def improve(row, column, data):\n",
    "    try: \n",
    "        id = int(row['PlayerId'])\n",
    "        season = int(row['Season'])\n",
    "        column = column\n",
    "        imp = float(data[(data['PlayerId'] == id) & (data['Season'] == season)][column]) > float(data[(data['PlayerId'] == id) & (data['Season'] == season-1)][column])\n",
    "\n",
    "        if imp:\n",
    "            return 'Yes' \n",
    "        return 'No'\n",
    "    except:\n",
    "        return 'Yes'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Killen\\AppData\\Local\\Temp\\ipykernel_44648\\3934428695.py:7: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  imp = float(data[(data['PlayerId'] == id) & (data['Season'] == season)][column]) > float(data[(data['PlayerId'] == id) & (data['Season'] == season-1)][column])\n",
      "C:\\Users\\Killen\\AppData\\Local\\Temp\\ipykernel_44648\\3934428695.py:7: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  imp = float(data[(data['PlayerId'] == id) & (data['Season'] == season)][column]) > float(data[(data['PlayerId'] == id) & (data['Season'] == season-1)][column])\n",
      "C:\\Users\\Killen\\AppData\\Local\\Temp\\ipykernel_44648\\3934428695.py:7: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  imp = float(data[(data['PlayerId'] == id) & (data['Season'] == season)][column]) > float(data[(data['PlayerId'] == id) & (data['Season'] == season-1)][column])\n",
      "C:\\Users\\Killen\\AppData\\Local\\Temp\\ipykernel_44648\\3934428695.py:7: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  imp = float(data[(data['PlayerId'] == id) & (data['Season'] == season)][column]) > float(data[(data['PlayerId'] == id) & (data['Season'] == season-1)][column])\n",
      "C:\\Users\\Killen\\AppData\\Local\\Temp\\ipykernel_44648\\3934428695.py:7: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  imp = float(data[(data['PlayerId'] == id) & (data['Season'] == season)][column]) > float(data[(data['PlayerId'] == id) & (data['Season'] == season-1)][column])\n",
      "C:\\Users\\Killen\\AppData\\Local\\Temp\\ipykernel_44648\\3934428695.py:7: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  imp = float(data[(data['PlayerId'] == id) & (data['Season'] == season)][column]) > float(data[(data['PlayerId'] == id) & (data['Season'] == season-1)][column])\n",
      "C:\\Users\\Killen\\AppData\\Local\\Temp\\ipykernel_44648\\3934428695.py:7: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  imp = float(data[(data['PlayerId'] == id) & (data['Season'] == season)][column]) > float(data[(data['PlayerId'] == id) & (data['Season'] == season-1)][column])\n",
      "C:\\Users\\Killen\\AppData\\Local\\Temp\\ipykernel_44648\\3934428695.py:7: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  imp = float(data[(data['PlayerId'] == id) & (data['Season'] == season)][column]) > float(data[(data['PlayerId'] == id) & (data['Season'] == season-1)][column])\n",
      "C:\\Users\\Killen\\AppData\\Local\\Temp\\ipykernel_44648\\3934428695.py:7: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  imp = float(data[(data['PlayerId'] == id) & (data['Season'] == season)][column]) > float(data[(data['PlayerId'] == id) & (data['Season'] == season-1)][column])\n"
     ]
    }
   ],
   "source": [
    "#Engineer Improve Features\n",
    "feats_to_improve = ['FUM', 'INT', 'PD', 'QB HUR', 'SACKS', 'SOLO', 'Pct_Team_INT', 'Pct_Team_SACKS', 'Pct_Team_TFL']\n",
    "\n",
    "for feat in feats_to_improve:\n",
    "    defense[feat+'_improve'] = defense.apply(lambda x: improve(x, feat, defense), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify Relevant Columns\n",
    "#No team stats added as of right now\n",
    "rel_feats = ['FUM', 'INT', 'PD', 'QB HUR', 'SACKS', 'SOLO', 'Pct_Team_INT', 'Pct_Team_SACKS', 'Pct_Team_TFL',\n",
    "             'FUM_improve', 'INT_improve', 'PD_improve', 'QB HUR_improve', 'SACKS_improve', 'SOLO_improve', \n",
    "             'Pct_Team_INT_improve', 'Pct_Team_SACKS_improve', 'Pct_Team_TFL_improve', \n",
    "             'Team','Conference','Position','Yr','Stars', 'Coach Change', 'Ranking', 'Rating', 'Transfer_Portal']\n",
    "\n",
    "\n",
    "#Get rid of 2019, narrow down to relevant features\n",
    "defense = defense[defense['Season'] != 2019]\n",
    "defense = defense[rel_feats]\n",
    "#Encode Label Columns\n",
    "defense['Transfer_Portal'] = np.where(defense['Transfer_Portal'].values == 'Yes', 1, 0)\n",
    "\n",
    "#Convert Yr and Stars to Categorical Variables\n",
    "defense['Yr'] = defense['Yr'].astype('str')\n",
    "defense['Stars'] = defense['Stars'].astype('str')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into Training and Testing Data\n",
    "X = defense.copy().drop(columns=['Transfer_Portal'])\n",
    "y = defense['Transfer_Portal']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n",
    "\n",
    "#Specify the Numerical Features and Categorical Features\n",
    "categorical = ['FUM_improve', 'INT_improve', 'PD_improve', 'QB HUR_improve', 'SACKS_improve', 'SOLO_improve', \n",
    "             'Pct_Team_INT_improve', 'Pct_Team_SACKS_improve', 'Pct_Team_TFL_improve', \n",
    "             'Team','Conference','Position','Yr','Stars', 'Coach Change']\n",
    "\n",
    "numerical = [feat for feat in X.columns if feat not in categorical]\n",
    "\n",
    "#Pipeline to SimpleImpute and OneHot Encode Categorical Features (Training data only)\n",
    "impute_encode = Pipeline([('impute',SimpleImputer(strategy='constant',fill_value='N/A')), ('encode',OneHotEncoder(handle_unknown='ignore'))])\n",
    "column_transform = ColumnTransformer([('cat_encode', impute_encode, categorical), ('numerical_pass', SimpleImputer(strategy='constant',fill_value=0),numerical)])\n",
    "\n",
    "X_train = column_transform.fit_transform(X_train)\n",
    "\n",
    "#Fit Pipeline ColumnTransformer to testing features\n",
    "X_test = X_test.fillna(0)\n",
    "X_test = column_transform.transform(X_test)\n",
    "\n",
    "#SMOTE Balancing of Training Data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train,y_train)\n",
    "X_train = X_train.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Scores:\n",
      "0.7257747485937411\n",
      "0.8110094529689867\n",
      "0.7155216775825675\n",
      "0.662165668623223\n",
      " \n",
      "Precision Scores:\n",
      "0.9293978955883997\n",
      "0.7588149154727054\n",
      "0.9335990717357177\n",
      "0.6166829720328392\n",
      " \n",
      "Recall Scores:\n",
      "0.6678453585987832\n",
      "0.9243053649050295\n",
      "0.6609916160936569\n",
      "0.8889799232544549\n"
     ]
    }
   ],
   "source": [
    "#CV Models w/ Training Data\n",
    "cv = KFold(n_splits = 5)\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def classifiers():\n",
    "    gb_clf = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=42))\n",
    "    SVM_clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "    forest_clf = RandomForestClassifier(random_state=42)\n",
    "    log_clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=10000, class_weight='balanced'))\n",
    "\n",
    "    gb_f1 = cross_val_score(gb_clf, X_train, y_train, cv=cv, scoring='f1')\n",
    "    SVM_f1 = cross_val_score(SVM_clf, X_train, y_train, cv=cv, scoring='f1')\n",
    "    forest_f1 = cross_val_score(forest_clf, X_train, y_train, cv=cv, scoring='f1')\n",
    "    log_f1 = cross_val_score(log_clf, X_train, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "    gb_pre = cross_val_score(gb_clf, X_train, y_train, cv=cv, scoring='precision')\n",
    "    SVM_pre = cross_val_score(SVM_clf, X_train, y_train, cv=cv, scoring='precision')\n",
    "    forest_pre = cross_val_score(forest_clf, X_train, y_train, cv=cv, scoring='precision')\n",
    "    log_pre = cross_val_score(log_clf, X_train, y_train, cv=cv, scoring='precision')\n",
    "\n",
    "    gb_re = cross_val_score(gb_clf, X_train, y_train, cv=cv, scoring='recall')\n",
    "    SVM_re = cross_val_score(SVM_clf, X_train, y_train, cv=cv, scoring='recall')\n",
    "    forest_re = cross_val_score(forest_clf, X_train, y_train, cv=cv, scoring='recall')\n",
    "    log_re = cross_val_score(log_clf, X_train, y_train, cv=cv, scoring='recall')\n",
    "\n",
    "    print('F1 Scores:')\n",
    "    print(gb_f1.mean())\n",
    "    print(SVM_f1.mean())\n",
    "    print(forest_f1.mean())\n",
    "    print(log_f1.mean())\n",
    "    print(' ')\n",
    "    print('Precision Scores:')\n",
    "    print(gb_pre.mean())\n",
    "    print(SVM_pre.mean())\n",
    "    print(forest_pre.mean())\n",
    "    print(log_pre.mean())\n",
    "    print(' ')\n",
    "    print('Recall Scores:')\n",
    "    print(gb_re.mean())\n",
    "    print(SVM_re.mean())\n",
    "    print(forest_re.mean())\n",
    "    print(log_re.mean())\n",
    "\n",
    "classifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base RandomForestClassifier Metrics\n",
      "-----------------------------------\n",
      "F1 Score:  0.18666666666666668\n",
      "Precision Score:  0.8235294117647058\n",
      "Recall Score:  0.10526315789473684\n",
      "Accuracy:  0.9465381244522348\n"
     ]
    }
   ],
   "source": [
    "#Evaluate trained model on test data\n",
    "#Use 'weighted' f1 score, recall, precision since data is imbalanced\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42, class_weight = 'balanced')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print('Base RandomForestClassifier Metrics')\n",
    "print('-----------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base GradientBoostingClassifer Metrics\n",
      "---------------------------------------\n",
      "F1 Score:  0.3037974683544304\n",
      "Precision Score:  0.96\n",
      "Recall Score:  0.18045112781954886\n",
      "Accuracy:  0.9517966695880806\n"
     ]
    }
   ],
   "source": [
    "#Evaluate trained model on test data\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print('Base GradientBoostingClassifer Metrics')\n",
    "print('---------------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base LogisticRegressionClassifer Metrics\n",
      "---------------------------------------\n",
      "F1 Score:  0.2972222222222222\n",
      "Precision Score:  0.18228279386712096\n",
      "Recall Score:  0.8045112781954887\n",
      "Accuracy:  0.7782646801051709\n"
     ]
    }
   ],
   "source": [
    "#Evaluate trained model on test data\n",
    "#Use 'weighted' f1 score, recall, precision since data is imbalanced\n",
    "\n",
    "clf = LogisticRegression(max_iter=10000, class_weight='balanced')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print('Base LogisticRegressionClassifer Metrics')\n",
    "print('---------------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummmyClassifier Metrics\n",
      "-----------------------------------\n",
      "F1 Score:  0.5012811553692057\n",
      "Precision Score:  0.4981481481481482\n",
      "Recall Score:  0.5044538209095171\n",
      "Accuracy:  0.5032482598607889\n"
     ]
    }
   ],
   "source": [
    "#Dummy Classifier:\n",
    "\n",
    "dummy = make_pipeline(StandardScaler(), DummyClassifier(random_state=42, strategy='stratified'))\n",
    "\n",
    "dummy.fit(X_train, y_train)\n",
    "preds = dummy.predict(X_test)\n",
    "\n",
    "print('DummmyClassifier Metrics')\n",
    "print('-----------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sklearn Job-lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue to test and tune the logistic regression and gradient boost models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "parameters = {\"random_state\": [42],\n",
    "              \"penalty\": ['l2'],\n",
    "              \"solver\": ['lbfgs', 'liblinear'], \n",
    "              \"C\": [1, 5, 10, 100, 1000], \n",
    "              \"max_iter\": [100, 500, 10000, 15000],\n",
    "              \"class_weight\": ['balanced']}\n",
    "\n",
    "clf_log = GridSearchCV(LogisticRegression(), parameters, scoring='recall').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LogisticRegression Metrics\n",
      "---------------------------------------\n",
      "F1 Score:  0.3101449275362319\n",
      "Precision Score:  0.19210053859964094\n",
      "Recall Score:  0.8045112781954887\n",
      "Accuracy:  0.7914110429447853\n"
     ]
    }
   ],
   "source": [
    "preds = clf_log.predict(X_test)\n",
    "\n",
    "print('Best LogisticRegression Metrics')\n",
    "print('---------------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GradientBoosting Metrics\n",
      "---------------------------------------\n",
      "F1 Score:  0.2199546485260771\n",
      "Precision Score:  0.1295060080106809\n",
      "Recall Score:  0.7293233082706767\n",
      "Accuracy:  0.6985100788781771\n"
     ]
    }
   ],
   "source": [
    "#GradientBoosting\n",
    "parameters = {\"random_state\": [42],\n",
    "              \"learning_rate\": [0.01, 0.015, 0.02,0.05, 0.075, 0.1],\n",
    "              \"n_estimators\": np.arange(1,100,5),\n",
    "              \"max_features\":['sqrt', 'log2', None]}\n",
    "\n",
    "clf_gb = GridSearchCV(GradientBoostingClassifier(), parameters, scoring='recall').fit(X_train, y_train)\n",
    "\n",
    "preds = clf_gb.predict(X_test)\n",
    "\n",
    "print('Best GradientBoosting Metrics')\n",
    "print('---------------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RandomForest Metrics\n",
      "---------------------------------------\n",
      "F1 Score:  0.44559585492227977\n",
      "Precision Score:  0.7166666666666667\n",
      "Recall Score:  0.3233082706766917\n",
      "Accuracy:  0.9531113058720421\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "parameters = {\"random_state\": [42],\n",
    "              \"n_estimators\": np.arange(1,100,5),\n",
    "              \"max_features\":['sqrt', 'log2', None], \n",
    "              \"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "              \"class_weight\": ['balanced']}\n",
    "\n",
    "clf_rf = GridSearchCV(RandomForestClassifier(), parameters, scoring='recall').fit(X_train, y_train)\n",
    "\n",
    "preds = clf_rf.predict(X_test)\n",
    "\n",
    "print('Best RandomForest Metrics')\n",
    "print('---------------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the probabilities that are given back to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = defense[defense['Transfer_Portal'] == 1].drop(columns=['Transfer_Portal'])\n",
    "X_neg = defense[defense['Transfer_Portal'] == 0].drop(columns=['Transfer_Portal'])\n",
    "\n",
    "# Specify the Numerical Features and Categorical Features\n",
    "categorical = ['FUM_improve', 'INT_improve', 'PD_improve', 'QB HUR_improve', 'SACKS_improve', 'SOLO_improve', \n",
    "             'Pct_Team_INT_improve', 'Pct_Team_SACKS_improve', 'Pct_Team_TFL_improve', \n",
    "             'Team','Conference','Position','Yr','Stars', 'Coach Change']\n",
    "\n",
    "numerical = [feat for feat in X.columns if feat not in categorical]\n",
    "\n",
    "#Pipeline to SimpleImpute and OneHot Encode Categorical Features (Training data only)\n",
    "impute_encode = Pipeline([('impute',SimpleImputer(strategy='constant',fill_value='N/A')), ('encode',OneHotEncoder(handle_unknown='ignore'))])\n",
    "column_transform = ColumnTransformer([('cat_encode', impute_encode, categorical), ('numerical_pass', SimpleImputer(strategy='constant',fill_value=0),numerical)])\n",
    "\n",
    "X_pos = column_transform.fit_transform(X_pos).toarray()\n",
    "X_neg = column_transform.fit_transform(X_neg).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 119 features, but LogisticRegression is expecting 126 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m log_pos_proba \u001b[38;5;241m=\u001b[39m \u001b[43mclf_log\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pos\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m      2\u001b[0m log_neg_proba \u001b[38;5;241m=\u001b[39m clf_log\u001b[38;5;241m.\u001b[39mpredict_proba(X_neg)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m      4\u001b[0m gb_pos_proba \u001b[38;5;241m=\u001b[39m clf_gb\u001b[38;5;241m.\u001b[39mpredict_proba(X_pos)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\Killen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:569\u001b[0m, in \u001b[0;36mBaseSearchCV.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call predict_proba on the estimator with the best found parameters.\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \n\u001b[0;32m    552\u001b[0m \u001b[38;5;124;03mOnly available if ``refit=True`` and the underlying estimator supports\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03m    to that in the fitted attribute :term:`classes_`.\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    568\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Killen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1377\u001b[0m, in \u001b[0;36mLogisticRegression.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1369\u001b[0m ovr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1374\u001b[0m     )\n\u001b[0;32m   1375\u001b[0m )\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ovr:\n\u001b[1;32m-> 1377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_proba_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m     decision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X)\n",
      "File \u001b[1;32mc:\\Users\\Killen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:366\u001b[0m, in \u001b[0;36mLinearClassifierMixin._predict_proba_lr\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict_proba_lr\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Probability estimation for OvR logistic regression.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m    Positive class probabilities are computed as\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m    1. / (1. + np.exp(-self.decision_function(X)));\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m    multiclass is handled by normalizing that over all classes.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 366\u001b[0m     prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    367\u001b[0m     expit(prob, out\u001b[38;5;241m=\u001b[39mprob)\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prob\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Killen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:332\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    329\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    330\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 332\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\Killen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Killen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 119 features, but LogisticRegression is expecting 126 features as input."
     ]
    }
   ],
   "source": [
    "log_pos_proba = clf_log.predict_proba(X_pos)[1].mean()\n",
    "log_neg_proba = clf_log.predict_proba(X_neg)[1].mean()\n",
    "\n",
    "gb_pos_proba = clf_gb.predict_proba(X_pos)[1].mean()\n",
    "gb_neg_proba = clf_gb.predict_proba(X_neg)[1].mean()\n",
    "\n",
    "print('Logistic Regression probabilities of a player entering the transfer portal')\n",
    "print('---------------------------------------')\n",
    "print('Average probability of players who did enter: ', log_pos_proba)\n",
    "print('Average probability of players who did NOT enter: ', log_neg_proba)\n",
    "print(' ')\n",
    "print('GB probabilities of a player entering the transfer portal')\n",
    "print('---------------------------------------')\n",
    "print('Average probability of players who did enter: ', gb_pos_proba)\n",
    "print('Average probability of players who did NOT enter: ', gb_neg_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
