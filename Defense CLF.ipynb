{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset and Coaching Data\n",
    "data = pd.read_csv('Data/Final Dataset.csv')\n",
    "\n",
    "coaching_data = pd.read_csv('Data/Coaching Data.csv', skiprows = [0,1], skipfooter = 202)\n",
    "coaching_data = coaching_data.rename(columns = {'FBS Team': 'Team'})\n",
    "coaching_data = coaching_data[['Team','2019','2020','2021','2022','2023','2024']]\n",
    "coaching_data = coaching_data.melt(id_vars='Team', var_name = 'Season', value_name = 'Coach')\n",
    "coaching_data['Season'] = coaching_data['Season'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Coaching Change Function to Add Coaching Change Column to Data\n",
    "def coach_change(row, data):\n",
    "\n",
    "    team = str(row.Team)\n",
    "    season = int(row.Season)\n",
    "\n",
    "    try:\n",
    "    \n",
    "        curr_coach = data[(data['Team'] == team) & (data['Season'] == season)]['Coach']\n",
    "        curr_coach = curr_coach[curr_coach.index[0]]\n",
    "        next_coach = data[(data['Team'] == team) & (data['Season'] == season + 1)]['Coach']\n",
    "        next_coach = next_coach[next_coach.index[0]]\n",
    "\n",
    "        if curr_coach != next_coach:\n",
    "            return 'Yes'\n",
    "        return 'No'\n",
    "        \n",
    "    \n",
    "    except:\n",
    "        return 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add coaching change info to data\n",
    "data['Coach Change'] = data.apply(lambda x: coach_change(x, coaching_data), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RB', 'WR', 'QB', 'TE', 'LB', 'DB', 'OL', 'DL', 'CB', 'S', 'PK',\n",
       "       'LS', 'P', 'DT', 'DE', 'FB', 'C', 'OT', 'G', 'NT', 'ATH', 'OLB',\n",
       "       '?'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions = data.Position.unique()\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add recruiting info\n",
    "for i,year in enumerate(['2015','2016','2017','2018', '2019','2020', '2021', '2022', '2023']):\n",
    "    file = pd.read_csv('Data/Player Recruit Ranking/' + year + '.csv')\n",
    "    file.rename(columns = {'AthleteId': 'PlayerId', 'Year': 'Class of'}, inplace = True)\n",
    "    data = data.merge(file, on = 'PlayerId', how = 'left', suffixes = [None, '_' + str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Columns\n",
    "for i in range(0,9):\n",
    "    data['Stars'] = data['Stars'].combine_first(data['Stars_' + str(i)])\n",
    "    data.drop(columns = ['Stars_' + str(i)], inplace = True)\n",
    "for i in range(1,9):\n",
    "    data['Rating'] = data['Rating'].combine_first(data['Rating_' + str(i)])\n",
    "    data['Ranking'] = data['Ranking'].combine_first(data['Ranking_' + str(i)])\n",
    "    data['Class of'] = data['Class of'].combine_first(data['Class of_' + str(i)])\n",
    "    data.drop(columns = ['Rating_' + str(i)], inplace = True)\n",
    "    data.drop(columns = ['Ranking_' + str(i)], inplace = True)\n",
    "    data.drop(columns = ['Class of_' + str(i)], inplace = True)\n",
    "\n",
    "data['Yr'] = data['Season'] - data['Class of'] + 1\n",
    "data.drop(columns = ['Class of'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_groups = {'OL':['OL', 'NT', 'OT', 'G', 'C','FB'],\n",
    "                   'TE':['TE'],\n",
    "                   'QB':['QB'],\n",
    "                   'RB':['RB'],\n",
    "                   'WR':['WR'],\n",
    "                   'DL':['DT', 'DE', 'DL'],\n",
    "                   'DB':['DB', 'CB', 'S'],\n",
    "                   'LB':['LB'],\n",
    "                   'ST':['LS', 'P', 'PK']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate Data into Position Groups\n",
    "data_sets = {}\n",
    "for key in position_groups.keys():\n",
    "    data_sets[key] = data.copy()[data.copy()['Position'].isin(position_groups[key])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Defensive data as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL = data_sets['DL'].copy().drop(columns = ['PositionId', 'ConferenceId', 'TeamId'])\n",
    "DB = data_sets['DB'].copy().drop(columns = ['PositionId', 'ConferenceId', 'TeamId'])\n",
    "LB = data_sets['LB'].copy().drop(columns = ['PositionId', 'ConferenceId', 'TeamId'])\n",
    "\n",
    "defense = pd.concat([DL,DB,LB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Season', 'PlayerId', 'Player', 'Position', 'Team', 'Conference',\n",
       "       'Usage Overall', 'Usage Pass', 'Usage Rush', 'Usage FirstDown',\n",
       "       'Usage SecondDown', 'Usage ThirdDown', 'Usage StandardDowns',\n",
       "       'Usage PassingDowns', 'ATT', 'AVG', 'CAR', 'COMPLETIONS', 'FGA', 'FGM',\n",
       "       'FUM', 'INT', 'In 20', 'LONG', 'LOST', 'NO', 'PCT', 'PD', 'PTS',\n",
       "       'QB HUR', 'REC', 'SACKS', 'SOLO', 'TB', 'TD', 'TFL', 'TOT', 'XPA',\n",
       "       'XPM', 'YDS', 'YPA', 'YPC', 'YPP', 'YPR', 'Division', 'ExpectedWins',\n",
       "       'Total Games', 'Total Wins', 'Total Losses', 'Total Ties',\n",
       "       'ConferenceGames Games', 'ConferenceGames Wins',\n",
       "       'ConferenceGames Losses', 'ConferenceGames Ties', 'HomeGames Games',\n",
       "       'HomeGames Wins', 'HomeGames Losses', 'HomeGames Ties',\n",
       "       'AwayGames Games', 'AwayGames Wins', 'AwayGames Losses',\n",
       "       'AwayGames Ties', 'Team firstDowns', 'Team fourthDownConversions',\n",
       "       'Team fourthDowns', 'Team fumblesLost', 'Team fumblesRecovered',\n",
       "       'Team games', 'Team interceptionTDs', 'Team interceptionYards',\n",
       "       'Team interceptions', 'Team kickReturnTDs', 'Team kickReturnYards',\n",
       "       'Team kickReturns', 'Team netPassingYards', 'Team passAttempts',\n",
       "       'Team passCompletions', 'Team passesIntercepted', 'Team passingTDs',\n",
       "       'Team penalties', 'Team penaltyYards', 'Team possessionTime',\n",
       "       'Team puntReturnTDs', 'Team puntReturnYards', 'Team puntReturns',\n",
       "       'Team rushingAttempts', 'Team rushingTDs', 'Team rushingYards',\n",
       "       'Team sacks', 'Team tacklesForLoss', 'Team thirdDownConversions',\n",
       "       'Team thirdDowns', 'Team totalYards', 'Team turnovers', 'Stars',\n",
       "       'Transfer_Portal', 'Coach Change', 'Ranking', 'Rating', 'Yr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defense.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Engineer PCT Features\n",
    "defense['Pct_Team_INT'] = defense['INT']/defense['Team interceptions']\n",
    "defense['Pct_Team_SACKS'] = defense['SACKS']/defense['Team sacks']\n",
    "defense['Pct_Team_TFL'] = defense['TFL']/defense['Team tacklesForLoss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Improve Function\n",
    "def improve(row, column, data):\n",
    "    try: \n",
    "        id = int(row['PlayerId'])\n",
    "        season = int(row['Season'])\n",
    "        column = column\n",
    "        imp = float(data[(data['PlayerId'] == id) & (data['Season'] == season)][column]) > float(data[(data['PlayerId'] == id) & (data['Season'] == season-1)][column])\n",
    "\n",
    "        if imp:\n",
    "            return 'Yes' \n",
    "        return 'No'\n",
    "    except:\n",
    "        return 'Yes'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Engineer Improve Features\n",
    "feats_to_improve = ['FUM', 'INT', 'PD', 'QB HUR', 'SACKS', 'SOLO', 'Pct_Team_INT', 'Pct_Team_SACKS', 'Pct_Team_TFL']\n",
    "\n",
    "for feat in feats_to_improve:\n",
    "    defense[feat+'_improve'] = defense.apply(lambda x: improve(x, feat, defense), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify Relevant Columns\n",
    "#No team stats added as of right now\n",
    "rel_feats = ['FUM', 'INT', 'PD', 'QB HUR', 'SACKS', 'SOLO', 'Pct_Team_INT', 'Pct_Team_SACKS', 'Pct_Team_TFL',\n",
    "             'FUM_improve', 'INT_improve', 'PD_improve', 'QB HUR_improve', 'SACKS_improve', 'SOLO_improve', \n",
    "             'Pct_Team_INT_improve', 'Pct_Team_SACKS_improve', 'Pct_Team_TFL_improve', \n",
    "             'Team','Conference','Position','Yr','Stars', 'Coach Change', 'Ranking', 'Rating', 'Transfer_Portal']\n",
    "\n",
    "\n",
    "#Get rid of 2019, narrow down to relevant features\n",
    "defense = defense[defense['Season'] != 2019]\n",
    "defense = defense[rel_feats]\n",
    "#Encode Label Columns\n",
    "defense['Transfer_Portal'] = np.where(defense['Transfer_Portal'].values == 'Yes', 1, 0)\n",
    "\n",
    "#Convert Yr and Stars to Categorical Variables\n",
    "defense['Yr'] = defense['Yr'].astype('str')\n",
    "defense['Stars'] = defense['Stars'].astype('str')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split into Training and Testing Data\n",
    "# X = defense.drop(columns=['Transfer_Portal'])\n",
    "# y = defense['Transfer_Portal']\n",
    "\n",
    "# X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n",
    "\n",
    "# #Specify the Numerical Features and Categorical Features\n",
    "# categorical = ['FUM_improve', 'INT_improve', 'PD_improve', 'QB HUR_improve', 'SACKS_improve', 'SOLO_improve', \n",
    "#              'Pct_Team_INT_improve', 'Pct_Team_SACKS_improve', 'Pct_Team_TFL_improve', \n",
    "#              'Team','Conference','Position','Yr','Stars', 'Coach Change']\n",
    "\n",
    "# numerical = [feat for feat in X.columns if feat not in categorical]\n",
    "\n",
    "# #Pipeline to SimpleImpute and OneHot Encode Categorical Features (Training data only)\n",
    "# impute_encode = Pipeline([('impute',SimpleImputer(strategy='constant',fill_value='N/A')), ('encode',OneHotEncoder(handle_unknown='ignore'))])\n",
    "# column_transform = ColumnTransformer([('cat_encode', impute_encode, categorical), ('numerical_pass', SimpleImputer(strategy='constant',fill_value=0),numerical)])\n",
    "\n",
    "# X_train = column_transform.fit_transform(X_train)\n",
    "\n",
    "# #Fit Pipeline ColumnTransformer to testing features\n",
    "# X_test = X_test.fillna(0)\n",
    "# X_test = column_transform.transform(X_test)\n",
    "\n",
    "# #SMOTE Balancing of Training Data\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train, y_train = smote.fit_resample(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason I commented out the above section was because it led to me using only the X_train and y_train data for cross validation.\n",
    "In the below cell I used the entire dataset, and split the data later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = defense.drop(columns=['Transfer_Portal'])\n",
    "y = defense['Transfer_Portal']\n",
    "\n",
    "#Specify the Numerical Features and Categorical Features\n",
    "categorical = ['FUM_improve', 'INT_improve', 'PD_improve', 'QB HUR_improve', 'SACKS_improve', 'SOLO_improve', \n",
    "             'Pct_Team_INT_improve', 'Pct_Team_SACKS_improve', 'Pct_Team_TFL_improve', \n",
    "             'Team','Conference','Position','Yr','Stars', 'Coach Change']\n",
    "\n",
    "numerical = [feat for feat in X.columns if feat not in categorical]\n",
    "\n",
    "#Pipeline to SimpleImpute and OneHot Encode Categorical Features (Training data only)\n",
    "impute_encode = Pipeline([('impute',SimpleImputer(strategy='constant',fill_value='N/A')), ('encode',OneHotEncoder(handle_unknown='ignore'))])\n",
    "column_transform = ColumnTransformer([('cat_encode', impute_encode, categorical), ('numerical_pass', SimpleImputer(strategy='constant',fill_value=0),numerical)])\n",
    "\n",
    "X = column_transform.fit_transform(X)\n",
    "\n",
    "#SMOTE Balancing of Training Data\n",
    "smote = SMOTE(random_state=42)\n",
    "X, y = smote.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Scores:\n",
      "0.6115639851019453\n",
      "0.6614724094435026\n",
      "0.6531328813168102\n",
      "0.5646135407849021\n",
      " \n",
      "Precision Scores:\n",
      "0.5438576951350073\n",
      "0.5953573057288597\n",
      "0.585133210427884\n",
      "0.522724862852422\n",
      " \n",
      "Recall Scores:\n",
      "0.9456965275546295\n",
      "0.9663177877239593\n",
      "0.9421447638197037\n",
      "0.946865964122604\n"
     ]
    }
   ],
   "source": [
    "#CV Models w/ Training Data\n",
    "cv = KFold(n_splits = 5)\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def classifiers():\n",
    "    gb_clf = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=42))\n",
    "    SVM_clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "    forest_clf = RandomForestClassifier(random_state=42)\n",
    "    log_clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=10000, class_weight='balanced'))\n",
    "\n",
    "    gb_f1 = cross_val_score(gb_clf, X.toarray(), y, cv=cv, scoring='f1')\n",
    "    SVM_f1 = cross_val_score(SVM_clf, X.toarray(), y, cv=cv, scoring='f1')\n",
    "    forest_f1 = cross_val_score(forest_clf, X.toarray(), y, cv=cv, scoring='f1')\n",
    "    log_f1 = cross_val_score(log_clf, X.toarray(), y, cv=cv, scoring='f1')\n",
    "\n",
    "    gb_pre = cross_val_score(gb_clf, X.toarray(), y, cv=cv, scoring='precision')\n",
    "    SVM_pre = cross_val_score(SVM_clf, X.toarray(), y, cv=cv, scoring='precision')\n",
    "    forest_pre = cross_val_score(forest_clf, X.toarray(), y, cv=cv, scoring='precision')\n",
    "    log_pre = cross_val_score(log_clf, X.toarray(), y, cv=cv, scoring='precision')\n",
    "\n",
    "    gb_re = cross_val_score(gb_clf, X.toarray(), y, cv=cv, scoring='recall')\n",
    "    SVM_re = cross_val_score(SVM_clf, X.toarray(), y, cv=cv, scoring='recall')\n",
    "    forest_re = cross_val_score(forest_clf, X.toarray(), y, cv=cv, scoring='recall')\n",
    "    log_re = cross_val_score(log_clf, X.toarray(), y, cv=cv, scoring='recall')\n",
    "\n",
    "    print('F1 Scores:')\n",
    "    print(gb_f1.mean())\n",
    "    print(SVM_f1.mean())\n",
    "    print(forest_f1.mean())\n",
    "    print(log_f1.mean())\n",
    "    print(' ')\n",
    "    print('Precision Scores:')\n",
    "    print(gb_pre.mean())\n",
    "    print(SVM_pre.mean())\n",
    "    print(forest_pre.mean())\n",
    "    print(log_pre.mean())\n",
    "    print(' ')\n",
    "    print('Recall Scores:')\n",
    "    print(gb_re.mean())\n",
    "    print(SVM_re.mean())\n",
    "    print(forest_re.mean())\n",
    "    print(log_re.mean())\n",
    "\n",
    "classifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base RandomForestClassifier Metrics\n",
      "-----------------------------------\n",
      "F1 Score:  0.9713322091062394\n",
      "Precision Score:  0.9990089197224975\n",
      "Recall Score:  0.9451476793248945\n",
      "Accuracy:  0.9723897911832947\n"
     ]
    }
   ],
   "source": [
    "#Evaluate trained model on test data\n",
    "#Use 'weighted' f1 score, recall, precision since data is imbalanced\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42, class_weight = 'balanced')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print('Base RandomForestClassifier Metrics')\n",
    "print('-----------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base GradientBoostingClassifer Metrics\n",
      "---------------------------------------\n",
      "F1 Score:  0.9688781664656212\n",
      "Precision Score:  0.9980119284294234\n",
      "Recall Score:  0.9413970932958274\n",
      "Accuracy:  0.9700696055684455\n"
     ]
    }
   ],
   "source": [
    "#Evaluate trained model on test data\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train.toarray(), y_train)\n",
    "preds = clf.predict(X_test.toarray())\n",
    "\n",
    "print('Base GradientBoostingClassifer Metrics')\n",
    "print('---------------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base LogisticRegressionClassifer Metrics\n",
      "---------------------------------------\n",
      "F1 Score:  0.843859649122807\n",
      "Precision Score:  0.7927482488669139\n",
      "Recall Score:  0.9020159399906236\n",
      "Accuracy:  0.8348027842227378\n"
     ]
    }
   ],
   "source": [
    "#Evaluate trained model on test data\n",
    "#Use 'weighted' f1 score, recall, precision since data is imbalanced\n",
    "\n",
    "clf = LogisticRegression(max_iter=10000, class_weight='balanced')\n",
    "\n",
    "clf.fit(X_train.toarray(), y_train)\n",
    "preds = clf.predict(X_test.toarray())\n",
    "\n",
    "print('Base LogisticRegressionClassifer Metrics')\n",
    "print('---------------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummmyClassifier Metrics\n",
      "-----------------------------------\n",
      "F1 Score:  0.5012811553692057\n",
      "Precision Score:  0.4981481481481482\n",
      "Recall Score:  0.5044538209095171\n",
      "Accuracy:  0.5032482598607889\n"
     ]
    }
   ],
   "source": [
    "#Dummy Classifier:\n",
    "\n",
    "dummy = make_pipeline(StandardScaler(), DummyClassifier(random_state=42, strategy='stratified'))\n",
    "\n",
    "dummy.fit(X_train.toarray(), y_train)\n",
    "preds = dummy.predict(X_test.toarray())\n",
    "\n",
    "print('DummmyClassifier Metrics')\n",
    "print('-----------------------------------')\n",
    "print('F1 Score: ', f1_score(y_test, preds))\n",
    "print('Precision Score: ', precision_score(y_test, preds))\n",
    "print('Recall Score: ', recall_score(y_test, preds))\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearch/Optimize/Hyperparameter Tune\n",
    "\n",
    "#Sklearn Job-lib\n",
    "\n",
    "#Full player data from collegefootballapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue to test and tune the logistic regression and gradient boost models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
